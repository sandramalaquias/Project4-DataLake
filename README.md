# data-engineering-DataLake-AWS-S3
Sparkfy in AWS-S3 using pyspark


![](https://miro.medium.com/max/1400/1*l6ukY_v43LK9LB9fjV2f0g.png)


Introduction

A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.

You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.
Project Description

## Project Description

In this project, I applied what I've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. To complete the project, I will need to load data from S3, process the data into analytics tables using Spark, and load them back into S3. I'll deploy this Spark process on a cluster using AWS.

## Files

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.

>song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json


And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```
This dataset was distributed into songs table and artist table.

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![](https://video.udacity-data.com/topher/2019/February/5c6c15e9_log-data/log-data.png)
This dataset was distributed into time, users, and songs_play table.

## Entity Relationship Diagram (ERD) - Star Schema on DataLake

<a href="https://ibb.co/DDTg6GK"><img src="https://i.ibb.co/hRw1nX8/udacity.png" alt="udacity" border="0"></a>

Benefits of the Star Schema on DataLake

    It is extremely simple to understand and build.
    No need for complex joins when querying data.
    Accessing data is faster (because the engine doesnâ€™t have to join various tables to generate results).
    Simpler to derive business insights.
    Works well with certain tools for analytics, in particular, with OLAP systems that can create OLAP cubes from data stored.
    You can use other resources on AWS like AWS GLUE and AWS Athena to perform queries on S3


## Run the scripts
The scripts of this project consist in single etl.py

### The environment:
- configparser => if need AWS Credentials
- argparse +> if need param from spark-submit
- pyspark: SparkContext, SparkConf
- pyspark.sqk: SparkSession
- pyspark.sql.functions: udf, col, trim, count, year, month, dayofmonth, hour, weekofyear, date_format, dayofweek, year, month, dayofmonth, hour, weekofyear, date_format, dayofweek, from_unixtime, to_timestamp, expr, row_number
- pyspark.sql.types: StructType, StructField, StringType, IntegerType,BooleanType,DoubleType,LongType
	
### Run the ETL
To run this ETL, choose:

1. At local mode or docker mode
2. Using AWS EMR Cluster

## At local mode
spark-submit etl.py --inputpath s3a://udacity-dend --outputpath s3a://smmbucketudacity --configpath dl.cfg --localrun y

Configurations:
- --inputpath: partial path for input data. If missing, inputpath will be s3a://udacity-dend
- --outputpath: partial path for output data. If missing, inputpath will be s3a://smmbucketudacity 
- --configpath: path for AWS credencials in local
- --localrun: "y" => this script will be run on local mode

Configpath content:\
[AWS]\
 AWS_ACCESS_KEY_ID=`<your AWS Access Key>`\  
 AWS_SECRET_ACCESS_KEY=`<your AWS Secret Key>`\  
 AWS_SESSION_TOKEN=`<your AWS Token Key>`  (optional)

## Using AWS EMR Cluster
1. configure AWS CLI
2. Grant acess to AWS resources ([link](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs.html))
3. Copy elt.py for your S3 Bucket
3. Lauch EMR cluster using CLI. Example:\
aws emr create-cluster \
--name emr_udacity \
--release-label emr-5.36.0 \
--log-uri s3://`<<bucketName/logs>`\  
--applications Name=Spark Name=JupyterEnterpriseGateway  \
--ec2-attributes KeyName=`<yourEC2key-pair>`,SubnetId=`<yoursubnet>` \
--instance-type m5.xlarge \
--instance-count 3
4. Get access to masterNode using SSH (follow the instruction on AWS Console - Emr - Cluster)
5. spark-submit s3a://`<yourBucket>`/etl.py --outputpath s3a://`<yourBucketName>` 

Configurations:
- --inputpath: will be s3a://udacity-dend
- --outputpath: partial path for output data. If missing, inputpath will be s3a://smmbucketudacity 


## Why S3 DataLake
According to AWS [link](https://aws.amazon.com/products/storage/data-lake-storage/?nc1=h_ls) ""Solving big-data challenges with data lakes

Organizations of all sizes, in all industries, are using data lakes to transform data from a cost that must be managed, to a valuable business asset. Data lakes are foundational for making sense of data at an organizational level. Data lakes remove data silos, making it easier to analyze diverse datasets, while keeping data secure, and incorporating machine learning.

S3 data lake customers have access to numerous AWS analytics applications, AI/ML services and high-performance file systems. This means you can run numerous workloads across your data lake, without additional data processing or transfers to other stores. You can also bring your preferred third-party analytics and machine learning tools to your S3 data lake. 

<a href="https://ibb.co/4FfP2CW"><img src="https://i.ibb.co/0nXhK1q/Screenshot-2022-08-01-at-17-10-42-Data-Lakes-Storage-AWS.png" alt="Screenshot-2022-08-01-at-17-10-42-Data-Lakes-Storage-AWS" border="0"></a>

#### More in this project
To understanding the spark decision in depth, see `etl.py'

## Data Analytics
The results will be write like parque files at AWS-S3.
The data Analytics will be easier to be done using AWS Glue and AWS Athena.

Using this ERD and their tables, is an easier way to create queries in AWS Athena to answer the single question to analytics issues like:
- which level of user  (paid/free) listen  more musics
	- SELECT  level, count(*) as count from songsplay group by level order by count desc LIMIT 1

	
- how much songs are listening by weekday
	- SELECT  dayname, count(*) as count from time join songsplay on time.start_time = songsplay.start_time group by dayname, weekday order by weekday
	

- how much songs are listened by users  gender
	- SELECT users.gender, count(*) from songsplay join users on songsplay.user_id = users.user_id group by gender

Or make more advanced topics like predict user churn or music recommendation using AWS-S3 tables.
 

